{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieCode.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQFJp6rU2xTy"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pandas\n",
        "!{sys.executable} -m pip install numpy\n",
        "!{sys.executable} -m pip install nltk\n",
        "!{sys.executable} -m pip install matplotlib\n",
        "!{sys.executable} -m pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re # for regular expression\n",
        "import string\n",
        "import nltk \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import Counter \n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "iUx44fRr3ErI"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the seeting of the cells\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
      ],
      "metadata": {
        "id": "bQydxqUF3QIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read file and save it in a data frame\n",
        "data_df=pd.read_excel('/content/datasetMovie.xltx') "
      ],
      "metadata": {
        "id": "gOib8BW23OqG"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ExNjKt5SwQwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "XMJy5Zwmb3eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_df)"
      ],
      "metadata": {
        "id": "_5FSZIqt3TKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.columns"
      ],
      "metadata": {
        "id": "jMNufr52Bc_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.count()"
      ],
      "metadata": {
        "id": "DwDe_b5NBrm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "bj-bmQpqAbUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.describe()"
      ],
      "metadata": {
        "id": "z_EPvwP2CCSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head()"
      ],
      "metadata": {
        "id": "RKZPj9y53X-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "WO2OX_OTcKCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuations  \n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "#this funcation take txet as input and return the text after removing punctuations \n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)  "
      ],
      "metadata": {
        "id": "5GbTBwM077U-"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.sample(3)"
      ],
      "metadata": {
        "id": "0o6OPz1QvTSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_arabic(text):\n",
        "    \n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    text = re.sub(\"ؤ\", \"و\", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Fwvykf1q79hQ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.sample(3)"
      ],
      "metadata": {
        "id": "cUTHiunUzh1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "def processPost(tweet):\n",
        "    #Replace @username with empty string\n",
        "    tweet = re.sub('@[^\\s]+', ' ', tweet)\n",
        "    \n",
        "    #Convert www.* or https?://* to \" \"\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet)\n",
        "    \n",
        "    #Replace #word with word\n",
        "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
        "\n",
        "    # remove punctuations\n",
        "    tweet= remove_punctuations(tweet)\n",
        "    \n",
        "    # normalize the tweet\n",
        "    #check if the word in the dictionary list (it have two letters as part of the word)\n",
        "    tweet= normalize_arabic(tweet)\n",
        "    \n",
        "    \n",
        "    #remove numbers\n",
        "    tweet = ''.join(i for i in tweet if not i.isdigit())\n",
        "    #remove english letters\n",
        "    tweet= re.sub(r'[a-z]+',\" \", tweet)\n",
        "    tweet= re.sub(r'[A-Z]+',\" \", tweet)\n",
        "    \n",
        "    \n",
        "    return tweet"
      ],
      "metadata": {
        "id": "q4tTbOxC8Bp7"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[\"clean text\"] =data_df[\"Text\"].apply(lambda x: processPost(x))"
      ],
      "metadata": {
        "id": "BurgIkin8GVx"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[\"text length\"] = data_df[\"clean text\"].apply(len)"
      ],
      "metadata": {
        "id": "RpU54hf3QM2l"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.sample(3)"
      ],
      "metadata": {
        "id": "CV5RblRKzs4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "data_df[\"tokens\"] = data_df[\"clean text\"].apply(tokenizer.tokenize)"
      ],
      "metadata": {
        "id": "x6tjz_9FDeyH"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head()"
      ],
      "metadata": {
        "id": "yYGjwTYYDhT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = [word for tokens in data_df[\"tokens\"] for word in tokens]\n",
        "sentence_lengths = [len(tokens) for tokens in data_df[\"tokens\"]]\n",
        "\n",
        "VOCAB = sorted(list(set(all_words)))\n",
        "\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(sentence_lengths))"
      ],
      "metadata": {
        "id": "8pLFsQ5vDtIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of words\n",
        "word_counter = Counter(all_words)"
      ],
      "metadata": {
        "id": "_OK3bYt2DyHA"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the most common words\n",
        "word_counter.most_common(10)"
      ],
      "metadata": {
        "id": "MGH08VIKD0tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 10 least common lines\n",
        "word_counter.most_common()[-10:]"
      ],
      "metadata": {
        "id": "T1ximap6D6LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive analysis"
      ],
      "metadata": {
        "id": "a2UOyQrAcx20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.shape"
      ],
      "metadata": {
        "id": "u_2NHPMaLXgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.columns"
      ],
      "metadata": {
        "id": "TLuOsZB6Mf1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info(verbose = True)"
      ],
      "metadata": {
        "id": "4JiTFSReN_Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[['Text','classification']].describe()"
      ],
      "metadata": {
        "id": "55iNiJCvnO3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.describe()"
      ],
      "metadata": {
        "id": "yiifif7tCn4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#libraries for word occurrence()\n",
        "from sklearn.feature_extraction.text  import TfidfTransformer\n",
        "from sklearn.feature_extraction.text  import CountVectorizer"
      ],
      "metadata": {
        "id": "gcAbwdrKO6jD"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate CountVectorizer()\n",
        "countVec = CountVectorizer()\n",
        "\n",
        "#generate word counts for the words\n",
        "word_count_vector = countVec.fit_transform(data_df['Text'].astype('U'))\n",
        "word_count_vector.shape"
      ],
      "metadata": {
        "id": "gTwEaCHSQFTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform a count matrix to a normalized tf-idf representation \n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "#idf values\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "metadata": {
        "id": "oxTb_8lOatbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print idf values\n",
        "df_idf = pd.DataFrame(tfidf_transformer.idf_, index = countVec.get_feature_names(),columns = [\"idf_weights\"])"
      ],
      "metadata": {
        "id": "4ZJ6xeFSbnP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#most frequent terms\n",
        "df_idf.sort_values(by=[\"idf_weights\"]).head(10)"
      ],
      "metadata": {
        "id": "ysa9o9TpcQxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#least frequent terms\n",
        "df_idf.sort_values(by=[\"idf_weights\"]).tail(10)"
      ],
      "metadata": {
        "id": "6g0oWdBddKku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predective Statistics"
      ],
      "metadata": {
        "id": "F1l35f3Leebi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
        "from sklearn.metrics  import confusion_matrix, classification_report\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "G4oXJx18onHy"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove data with NAN stance\n",
        "data_df=data_df[~data_df['classification'].isna()]"
      ],
      "metadata": {
        "id": "FucwVNxK4bhU"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the \"Neutral\" class\n",
        "data_df=data_df[data_df['classification'] != \"Neutral\"]"
      ],
      "metadata": {
        "id": "Z-6brP3P8fKY"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head(5)"
      ],
      "metadata": {
        "id": "t5Sib8x7-3Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change values to numeric\n",
        "data_df['classification'] = data_df['classification'].map({'Positive':1, 'Negative':0 }) "
      ],
      "metadata": {
        "id": "NYSY5ZrG_AkP"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head(5)"
      ],
      "metadata": {
        "id": "cjhAIBD4vm0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# idneitfy the data and the labels\n",
        "data= data_df['clean text']\n",
        "target= data_df['classification']"
      ],
      "metadata": {
        "id": "jpQNr51rBr-X"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TfidfVectorizer for feature extraction (TFIDF to convert textual data to numeric form):\n",
        "tf_vec = TfidfVectorizer()\n",
        "X = tf_vec.fit_transform(data)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "DaqM1E23D5lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Phase\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.50, random_state=0)"
      ],
      "metadata": {
        "id": "yUn1xMNxJW0d"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training: \", X_train.shape, y_train.shape)\n",
        "print(\"Testing: \", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "VoPPlkszNIgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier: Naive Bayes\n"
      ],
      "metadata": {
        "id": "cWa7gfqktEE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the classifer and fit the training data and lables\n",
        "classifier_nb = MultinomialNB().fit(X_train.todense(),y_train)\n",
        "\n",
        "print(\"MultinomialNB accuracy: %.2f\"%classifier_nb.score(X_test.todense(), y_test))\n",
        "print('_'*100)\n",
        "\n",
        "#do a 10 fold cross-validation \n",
        "results_nb = cross_val_score(classifier_nb, X.todense(),target, cv=10)\n",
        "print(\"\\n10-fold cross-validation:\")\n",
        "print(results_nb)\n",
        "\n",
        "print(\"The average accuracy of the MultinomialNB classifier is : %.2f\" % np.mean(results_nb))\n",
        "print('_'*100)\n",
        "\n",
        "print(\"\\nConfusion matrix of the MultinomialNB classifier:\")\n",
        "predicted_nb = classifier_nb.predict(X_test.todense())\n",
        "print(confusion_matrix(y_test,predicted_nb))\n",
        "print('_'*100)\n",
        "\n",
        "print(\"\\nClassification_report of MultinomialNB classifier:\")\n",
        "print(classification_report(y_test,predicted_nb))\n",
        "print('_'*100)"
      ],
      "metadata": {
        "id": "b5W-k5PduS75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "probs = classifier_nb.predict_proba(X_test)\n",
        "preds = probs[:,1]\n",
        "\n",
        "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
        "roc_auc = metrics.auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "Zd7P_g6AybxY"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# polt the AUC\n",
        "plt.title('Receiver Operating Characteristic SVM classifier')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r5h5xnVE0oyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}